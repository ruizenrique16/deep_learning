# Modelos de Deep Learning para Clasificaci√≥n de Im√°genes y Texto

Este repositorio contiene una serie de notebooks que exploran distintos **modelos de Deep Learning** aplicados a problemas de **clasificaci√≥n**, tanto en im√°genes como en texto.  
El objetivo es comparar el desempe√±o de arquitecturas cl√°sicas (CNN, LSTM) frente a modelos basados en **Transformers** y **fine-tuning** de redes preentrenadas.

-----------------------------------------------------------------------------------------------------

## Estructura del proyecto

‚îú‚îÄ‚îÄ notebooks/ # Notebooks con los experimentos principales
‚îÇ ‚îú‚îÄ‚îÄ 01_CNN_clasificacion.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 02_LSTM_vs_Transformer.ipynb
‚îÇ ‚îî‚îÄ‚îÄ 03_Transformer_Fine_Tuning.ipynb
‚îÇ
‚îú‚îÄ‚îÄ results/ # Resultados generados por cada notebook (CSV, m√©tricas, gr√°ficas)
‚îÇ
‚îî‚îÄ‚îÄ docs/ # Reportes t√©cnicos en formato Markdown (.md)


-----------------------------------------------------------------------------------------------------

## Contenido de los experimentos

### 1. Clasificaci√≥n de im√°genes con CNN
- **Dataset:** CIFAR-10  
- **T√©cnicas:** normalizaci√≥n, aumento de datos, regularizaci√≥n L2, *EarlyStopping*.  
- **Modelo:** Red Convolucional profunda (*CNN*) para clasificaci√≥n en 10 categor√≠as.  
- **Salida:** precisi√≥n, p√©rdida, matriz de confusi√≥n y reporte de clasificaci√≥n.  

Reporte t√©cnico: `docs/01_CNN_clasificacion.md`  
Resultados:      `results/cnn_results.csv`

-----------------------------------------------------------------------------------------------------

### 2. Modelos secuenciales ‚Äî LSTM vs Transformer
- **Dataset:** IMDb (rese√±as de pel√≠culas).  
- **T√©cnicas:** tokenizaci√≥n, *padding*, embeddings, comparaci√≥n de arquitecturas secuenciales.  
- **Modelos:** LSTM y Transformer implementados con Keras/TensorFlow.  
- **Salida:** m√©tricas de evaluaci√≥n, matrices de confusi√≥n y resultados comparativos.  

Reporte t√©cnico: `docs/02_LSTM_vs_Transformer.md`  
Resultados: `results/lstm_transformer_results.csv`

-----------------------------------------------------------------------------------------------------

### 3. Fine-tuning de Transformer preentrenado (DistilBERT)
- **Dataset:** IMDb.  
- **Modelo:** `distilbert-base-uncased` (Hugging Face Transformers).  
- **T√©cnicas:** ajuste fino (*fine-tuning*) y evaluaci√≥n con `Trainer` de ü§ó Transformers.  
- **Comparaci√≥n:** desempe√±o frente a LSTM y Transformer del experimento anterior.  

Reporte t√©cnico: `docs/03_Transformer_Fine_Tuning.md`  
Resultados: `results/fine_tuning_results.csv`

-----------------------------------------------------------------------------------------------------

## Requisitos

Para ejecutar las notebooks se recomienda crear un entorno con las siguientes librer√≠as:

```bash
pip install tensorflow torch transformers datasets evaluate scikit-learn seaborn matplotlib pandas numpy
```

Se sugiere usar Python 3.10+ y ejecutar las notebooks desde Jupyter o Google Colab.

## Ejecuci√≥n

Abrir la carpeta notebooks/

Ejecutar cada notebook en orden:

01_CNN_clasificacion.ipynb

02_LSTM_vs_Transformer.ipynb

03_Transformer_Fine_Tuning.ipynb

‚úíÔ∏è Autor
Pedro Enrique Ruiz Riveros
üìß Contacto: [enriq16@fpuna.edu.py]
üìö Portafolio de proyectos de Deep Learning y NLP